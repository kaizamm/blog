---
layout: post
published: true
title:  ceph基本操作
categories: [document]
tags: [ceph,存储]
---
* content
{:toc}

### 集群操作
通过systemd来管理
```
systemctl start ceph.target #start all daemons
systemctl stop ceph*.service ceph*.target
systemctl status ceph-osd@12 #检查osd.12的状态
systemctl status ceph*.service ceph*.target
```
按种类停止/启动一类daemon
```
systemctl stop ceph-mon\*.service ceph-mon.target
systemctl stop ceph-osd\*.service ceph-osd.target
systemctl stop ceph-mds\*.service ceph-mds.target

systemctl start/stop ceph-osd@{id}
systemctl start/stop ceph-mon@{hostname}
systemctl start/stop ceph-mds@{hostname}
```

### 健康检查

### 监控集群
```
ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status

ceph -s
```
除了每个守护进程有自己的日志，mointor上面的<font color=pink>/var/log/ceph/ceph.log</font>记录了整个系统高级别事件，同时也可以通过 <font color=pink>ceph -w </font>来监控。同时也可以用<font color="pink">ceph log last [n]</font>来查看最近n行的日志。

查看集群数据使用量以及在pool间的分布情况，用df命令，有点类似Linux的df <font bgcolor="pink">ceph df</font>

查看osd的状态  
osd的状态要么是在集群中（in the cluster）或是不在集群中（out）；并且，要么是正在运行的（up）或是停止运行的（down），如果在cluster中，就可读写；如果在它在集群中并且即将移除集群，ceph就会迁移pg到其他的osd；如果osd不在集群中(out)，crush将不会将pg绑到该osd上；如果osd是“out”的，那么它的状态也应该是out的；总而言之，它的状态有两个集合(in|out)和（up|down）

```
ceph osd stat
ceph osd dump
ceph osd tree
```
查看monitor集quorum状态
```
ceph quorum_status|python -m json.tool
```
查看Md5状态  
mds提供文件存储
```
ceph mds stat
#for i in node{2..4};do ssh $i sudo ceph mds stat;done
```
查看pg(placement group)状态  
pg映射对象到osd中

通过admin socket来查询  
路径为/var/run/ceph,ceph daemon {daemon-name} help


### 用户管理
ceph auth ls
```
[ceph@node1 my-cluster]$ ssh node2 sudo ceph auth ls
mds.node2
	key: AQBLa4Bc+IOQEBAAWniShT3OmiN/m7J6FDTKog==
	caps: [mds] allow
	caps: [mon] allow profile mds
	caps: [osd] allow rwx
osd.0
	key: AQCci39cdpzYERAA65jsvRJDbW1VIaTN/Tjxvw==
	caps: [mgr] allow profile osd
	caps: [mon] allow profile osd
	caps: [osd] allow *
  ...
```
输出格式：type.id，以上mds是类型，node2是id;osd是类型，0是id；接着是key,value认证，caps定义了权限范围

查单个用户

ceph auth get {type.id}

加一个用户

```
ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring
ceph auth get-or-create-key client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key
```
如果只提供了用户的osd权限，而没有限制进入特定pool，用户就有进入所有集群的Pools的权限。

删一个用户
...

keyring管理   
默认位置：/etc/ceph/$cluster.$name.keyring /etc/ceph/$cluster.keyring /etc/ceph/keyring /etc/ceph/keyring.bin


### Pools

首先什么是POOLS?

当你部署了一个集群，却并没有创建一个pool时，ceph会使用默认的pool来存储数据。一个pool能给你提供：
1. Resilience: 还原能力，你可以设置允许多少OSD来避免丢失数据，对于replicated pools，一个对象的副本是固定数量。典型的配置是size=2,只有一个副本。

2. Placement Groups：你可以为这个Pool设置placement groups的数量。一个典型的配置是一个osd，100个placement，来提供最佳的平衡避免使用太多的计算资源。当有多个pool时，要保证给pool整体上有一个合理数量的 placement groups

3. CRUSH rules
4. snapshots: 快照：ceph osd pool mksnap，其实是对部分pool做了快照

> 从Luminous开始，所有的pools需要绑定到应用

list
```
ceph osd lspools
```
创建
```
ceps osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] \
     [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} {pg-num}  {pgp-num}   erasure \
          [erasure-code-profile] [crush-rule-name] [expected_num_objects
```
<font color=green>绑定pool到应用</font>    
pool在使用之前需要与应用绑定。文件系统的pools和通过RGW对象存储自动创建的pools都会自动绑定；块存储（RBD）的pools，需要通过rbd工具对它初始化。  
其他场景，你可以手动绑定一个自由格式应用名到一个Pool

```
ceph osd pool application enable {pool-name} {application-name}
```
>注意：<font color=red>CephFs的应用名application name是cephfs,RBD的应用名是rbd，RGW的应用名是rgw</font>

设置pool quota

删除pool

重命名pool

查看pool的统计
```
rados df
```
快照

移除快照

给Pool设置一个Key/value

获取pool的value

###
