---
layout: post
published: true
title:  ceph体系结构
categories: [document]
tags: [存储,ceph]
---
* content
{:toc}

### 前言
最近应该会有段时间不忙，研究下ceph，说到存储，之前做过总结，但总不能沉到下面的技术中去，一直浮在上面的概念上面，希望能借助于本节对ceph做个拓展；  
存储关键字：nas san das lvs 块存储 分布式存储 对象存储 文件存储 ceph  
初步理解：根据客户端，有三种接口：
1. 块存储：如裸设备存储，DAS，即直接连接存储，如磁盘就是这种，SAN,存储区域网络，磁盘阵列也是这种，在存储时，将数据分割成小的数据块；引申：磁盘的存储方式：扇区、block size、簇；扇区是最小的存储单元，但是系统在存取或读取一个文件时，不可能只读一个扇区，而是以block为最小单位；  
2. 文件存储：主要是cifs或是nfs协议，有文件服务器，传输速度快，如ftp；主要是通过网络共享，nas  
3. 对象存储：对象存储将数据与元数据分离，改变了传统数据存储的方式，传统的存储有点类似链表形式，需要进行遍历；而对象存储则是将元数据分离后统一进行存储，那么在访问某个文件时，只需要拿到它的元数据，就能找到它的存储地址，从而进行访问；所谓元数据，是指这个存储对象的创建时间、访问时间、大小等，说白了就是inode；能通过inode来索引文件，关于inode的详细信息，可以参考[阮一峰的网络日志](http://www.ruanyifeng.com/blog/2011/12/inode.html)  
> 关于硬链接，Inode号相同的，文件名不同的两个文件，修改一个文件中的内容，可以导致另一个文件中的内容同时发生变化，而删除这个文件，不会引起另一个文件的删除；有了inode，可以删除那些通过文件名不好删除的文件；``` find . -inum 271761664  -exec rm -i {} \;```


主机应用访问存储模式

1. DAS  
传统部署存储方式，存储管理通过各自连接的主机进行，其它主机访问存储必须通过LAN共享

2. NAS  
是提供存储功能和文件系统的网络服务器。客户端可以访问NAS上的文件系统，还可以上传和下载文件。支持协议包括CIFS NFS SMB AFS等。对客户端来说，NAS就是一个网络上的文件服务器。

3. SAN  

  那么什么是磁盘阵列呢？
  ```
  它有两大部件：

  1.控制器  
  “大脑”，处理器和缓存，IO操作、Raid管理、数据管理：快照、镜像、复制等

  2.磁盘柜  
  磁盘扩展柜，包含多块磁盘
  ```
  它有以下几个特性
  ```
  1.计算（主机）、传输（交换机）、存储（RAID/Tape）分离  
  2.主机可以访问任何存储设备，存储设备之间可以互访  
  3.主机、存储设备可以独立扩展  
  ```
  它与NAS之间的区别是什么呢？  
  ```
  它和NAS之间的区别是SAN只提供了块存储，而把文件系统的抽象交给客户端来管理。SAN的客户端和服务端之间的协议有Fibre Channel、ISCSI、ATA over Ethernet(AoE)和HyperSCSI。对于客户端来说，SAN就是一块磁盘，可以对其格式化、创建文件系统并挂载。

  SAN和NAS并不对立的，现代的网络存储通常都是两者混合使用，可以同时提供文件级别的协议和块级别的协议。SAN和NAS就是传统的网络存储。

  对于网络文件系统，数据是一个个文件的形式来管理；对于块存储，数据是数据块的形式来管理，每个数据块有自己的地址。对象存储则是以一个对象的形式来管理数据，对象有三个元素：对象的数据、对象的元数据、全局惟一标识符ID
  ```

4. 对象存储  
REST API ：GET PUT POST DELETE  
> <font color="green">post与put的区别：put用于创建或替换某个网络资源，在替换某个网络资源时，如果其存在，则Post报错而不替换；Put则是直接替换。</font>

[ceph中文官网](http://docs.ceph.org.cn/architecture/)


### ceph架构


![ceph架构](/styles/images/ceph-stack.png)

ceph存储集群至少需要一个monitor,manager,OSD(Object Storage Daemon)，如果ceph文件系统客户端，也需要一个Metadata Server(MDSs)


1. Monitors: ceph-mon,主要是集群中的映射关系，如Monitor map，manager map,OSD map和CRUSH map;这些映射关系保证了守护进程之间的信息传递；mointor同时也负责守护进程和client之间的认证；为了保证高可用，至少需要三个mointor  

2. Managers: ceph-mgr,监控集群的状态，它有基于python的插件，提供Dashboad和rest API;同样为了保证高可用，至少要有两个manager  

3. Ceph OSDS：ceph-osd,将数据以对象的形式存储到节点；数据复制、数据恢复、维护集群之间的心跳；至少保证3个节点  

4. MDSs: ceph-mds,存储元数据，对象存储和块存储不使用MDS,只在文件存储时使用;metadata sever允许使用执行基本的命令，如：find,ls  

ceph存储数据时按逻辑存储池存储对象，利用CRUSH算法，它会计算哪个placement group来存储对象，并且会计算哪个OSD守护进程来存储placement group,这个算法来保证存储集群弹性伸缩、重新平衡和动态恢复

*ceph 条带化：条带化是独立于对象复制的，因为CRUSH会在OSD间复制对象，数据条带是自动被复制的。*


CEPH客户端:三种数据服务接口：RBD RGW(radosgw) CephFS。

![](/styles/images/ceph-architecture.png)

### librados

Ceph 存储集群提供了消息传递层协议，用于客户端与 ***Ceph 监视器*** 和 ***OSD*** 交互， librados 以库形式为 Ceph 客户端提供了这个功能。所有 Ceph 客户端可以用 librados 或 librados 里封装的相同功能和对象存储交互，例如 librbd 和 libcephfs 就利用了此功能。你可以用 librados 直接和 Ceph 交互（如和 Ceph 兼容的应用程序、你自己的 Ceph 接口、等等）


### ceph 多pool 多pg 对象复本 复本策略
ceph很强大的功能之一就是：能够绑定到多个不同的Pool，每个pool能有不同数量的pg,对象复本和复本策略。如，“ a pool could be set up as a “hot” pool that uses SSDs for frequently used objects or a “cold” pool that uses erasure coding”

### 数据存储
ceph存储集群从 **ceph客户** 端接收数据--不管是来自 **ceph块设备** 、还是 **ceph对象存储** 、**ceph文件系统**、还是基于librados的自定义实现--并存储为对象。每个对象是文件系统中的一个文件，它们存储在对象存储设备上。由ceph OSD守护进程处理存储设备上的读、写操作。

ceph OSD在扁平的命名空间内把所有数据存储为对象，也就是没有目录层次。对象包含 ***一个标识符*** 、***二进制数据*** 、和由名字/值对组成的 ***元数据***，元数据语义完全取决于ceph客户端。如cephFS用元数据存储文件属性、文件所有者、创建日期、修改日期等。***一个对象的id不仅在本地惟一且在全局惟一***

### ceph的伸缩性和可用性
ceph是分布式架构。在传统架构里，客户端与一个中心化组件通信（如网关、中间件、API、前端等），它作为一个统一入口，引入了单点故障，同时也限制和伸缩性（也就是说中心组件挂了，整个系统就挂了）；ceph消除了集中网关，允许客户端直接和ceph osd守护进程通讯。ceph osd守护进程自动在其他ceph节点创建对象副本来确保数据安全和高可用性。ceph monitor监控器也是集群的，保证高可用。为了消除中心化，引用crush算法。

### crush 算法
ceph客户端和osd守护进程都用crush算法来计算对象的位置信息。而不是依赖一个中心化的查询表。保证 ***伸缩性***。CRUSH用智能数据复制确保 ***弹性***，更能适应大规模存储。

### 集群运行图

ceph依赖ceph客户端和OSD，因为它们知道集群的拓扑，这个拓扑由5张图共同描述，统称为集群运行图。

1. Monitor Map:包含集群的fsid、位置、名字、地址和端口，也包括当前版本、创建时间、最近修改时间。要查看监控器图用`ceph mon dump`

2. OSD Map: fsid 创建时间 修改时间 存储池列表 副本数量 归置组数据 OSD列表及其状态(up in)。 `ceph osd map`

3. PG Map：归置组版本、时间戳、最新的OSD运行图版本、占满率、以及各归置组详情，如归置组ID、up set、acting set、PG状态（如：active+clean），和各存储池的数据使用情况统计。

4. CRUSH Map：存储设备列表、故障域树状结构（设备、主机、机架、行、房间等）、和存储数据如何利用此树状结构的规则。`ceph osd getcrushmap -o {filename}`，然后用 `crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}`反编译，然后就可以用cat查看就行了。

5. MDS Map：包含当前MDS图版本、创建时间、修改时间、元数据存储池、元数据服务器列表、还有哪些元数据服务器是up且in的。`ceph mds dump`


### cephx来认证用户和守护进程，共享密钥

### paxos算法就集群的状态达到一致，保证高可用

### 存储池

### 对应PG到OSD

每个pool有pg.CRUSH将PG动态地映射到OSD上。当ceph客户端存储对象时，CRUSH将每个对象映射到PG上。
