---
layout: post
published: true
title:  通过ceph-deploy安装ceph storage cluster
categories: [document]
tags: [存储]
---
* content
{:toc}

### 前言
[官网](http://docs.ceph.com/docs/master/start/)，ceph安装

### 实验环境
通过vagrant搭建4台机器node1~4  
其中node2,3,4作osd，在vagrant创建完的机器后，需要打开virtualbox各新增一块磁盘，sdb。
注意：本实验环境node2,3,4对应图中的1,2,3，本文中的node1是deploy机器，同时也是测试的机器   
vagrantfile如下：  
```bash
## files: Vagrantfile
# -*- mode: ruby -*-
# vi: set ft=ruby :
hosts = {
  "node1" => "192.168.0.11",
  "node2" => "192.168.0.12",
  "node3" => "192.168.0.13",
  "node4" => "192.168.0.14"
}

Vagrant.configure("2") do |config|
  config.vm.box = "bento/centos-7.2"
  config.vm.box_url = "./vagrant-centos-7.2.box"
  config.vm.provider "virtualbox" do |v|
    v.customize ["modifyvm",:id,"--memory",3072]
  end
  hosts.each do |name, ip|
    config.vm.define name do |nodes|
      nodes.vm.hostname = name
      nodes.vm.network :private_network, ip: ip
      nodes.vm.provision "shell",
        run: "always",
        inline: "sudo ifup enp0s8; export LC_ALL=en_US.UTF-8; export LANG=en_US.UTF-8"
      end
    end
  end
```

### ceph-deploy安装
```bash
yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

# 配置源，ceph-stable-release变量取值当前稳定版本，如luminous
cat << EOM > /etc/yum.repos.d/ceph.repo
[ceph-noarch]
name=Ceph noarch packages
#baseurl=https://download.ceph.com/rpm-{ceph-stable-release}/el7/noarch
baseurl=https://download.ceph.com/rpm-luminous/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
EOM

yum update
yum install ceph-deploy
yum install ntp ntpdate ntp-doc
yum install openssh-server

#创建一个ceph deploy user，配置无密码及sudo权限
#在每个节点执行
useradd ceph
echo 'ceph' | passwd --stdin ceph
echo "ceph ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/ceph
chmod 0440 /etc/sudoers.d/ceph
# 配置sshd可以使用password登录
sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
systemctl reload sshd
# 配置sudo不需要tty
sed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers

#配置hosts
cat > /etc/hosts<<EOF
192.168.0.14 node4 node4
192.168.0.13 node3 node3
192.168.0.12 node2 node2
192.168.0.11 node1 node1
EOF

#配免密
#登陆至node1
su - ceph
ssh-keygen
ssh-copy-id ceph@node2
ssh-copy-id ceph@node3
ssh-copy-id ceph@node4

#开防火墙端口
#配置完成后，记得service iptables save
#同理，如果是firewalld类似
# ceph moinitor:6789  
# ceph OSDS: 6800:7300
#iptables -A INPUT -i {iface} -p tcp -s {ip-address}/{netmask} --dport 6789 -j ACCEPT
或
#firewall-cmd --add-port 6789/tcp --permanent
#或者直接关闭iptables,firewalld,selinx
systemctl stop firewalld && systemctl disable firewalld
setenforce 0

```

至此，这准备阶段已经完成，现在看下这个部署架构  
![](/styles/images/ceph-deploy01.png)

由于是第一次搭建，先创建一个ceph存储集群：一个moinitor+3个ceph OSD。后续如果要扩展，再加第四个OSD，一个metadata 服务，两个mointor。



### 部署

```bash
su - ceph
mkdir my-cluster
cd my-cluster
#创建cluster,指定hostname
#如果在执行时提示无法找到pkg-packages时，需要yum install python-setuptools -y
ceph-deploy new node2 node3  node4
#执行ceph-deploy install node时，其实就是在对应结点上，安装ceph ceph-radosgw这两个包
ceph-deploy install node2 node3 node4
#部署monitor和生成keys
ceph-deploy mon create-initial
#复制配置文件和keyring到node节点，方便执行命令时不用指定monitor地址和keyring
ceph-deploy admin node2 node3 node4
ceph-deploy mgr create node2 #该条命令是Deploy a manager daemon. (Required only for luminous+ builds):
#创建三个osd，所谓osd，即存储介质，
ceph-deploy osd create --data /dev/sdb node2
ceph-deploy osd create --data /dev/sdb node3
ceph-deploy osd create --data /dev/sdb node4
#以下，如果在执行健康检查时候，一直没有health_ok，基本上就是时钟不对，需要开启时钟同步。
ssh node1 sudo ceph health
ssh node1 sudo ceph -s
```

### 清理集群

```
#至此，如果安装失败，需要清除，重新开始
ceph-deploy purge node2 node3 node4
ceph-deploy purgedata node2 node3 node4
ceph-deploy forgetkeys
rm ceph.*
```

### 扩展
基础的集群搭建完成后，需要扩展。在node2上增加一个metadata server，在node3和node4上加一个ceph monitor和manager；增加高可靠

![](/styles/images/ceph-deploy-expanding.png)
上图取自官网，对应实验环境的node2,3,4


```bash
#如果要用cephFS，即如果要用文件存储，需要安装这个mds
ceph-deploy mds create node2
#加Monitor，高可用需要，Paxos算法
ceph-deploy mon add node3
ceph-deploy mon add node4
#如果你加了ceph mointor，ceph将开始同步多个Monitor之间的数据，检查monitors状态
ceph quorum_status --format json-pretty
#增加managers，主从模式
ceph-deploy mgr create node3 node4
#查看standby manager，其实是查看ceph，所有的参数。
ssh node2 sudo ceph -s
#增加一个RGW 实例，监听7480
ceph-deploy rgw create node2
```

### 存取对象数据
在集群里面存一个对象数据，必须设置一个对象名字、指定一个pool。客户端检索最新的集群地图并通过CRUSH算法计算出对象到placement group的映射关系，并且动态计算出placement group到OSD之间的绑定。因此如果要找到对象的位置，你只需要知道<font color=red>object name及pool name</font>
```bash
echo {Test-data} > testfile.txt
ceph osd pool create mytest 8
rados put {object-name} {file-path} --pool=mytest
rados put test-object-1 testfile.txt --pool=mytest
rados -p mytest ls
ceph osd map {pool-name} {object-name}
ceph osd map mytest test-object-1
```

实验如下：         

```bash
echo "fuck fuck!!" > testfile.txt
#创建pool
[root@node2 ~]# ceph osd pool create mytest 8
pool 'mytest' created
#rados put {object name} {file-path} --pool=mytest，存数据
[root@node2 ~]# rados put test-object-1 testfile.txt --pool=mytest
#查看对应pool里的数据
[root@node2 ~]# rados -p mytest ls
test-object-1
#查看它的映射关系
[root@node2 ~]# ceph osd map mytest test-object-1
osdmap e31 pool 'mytest' (5) object 'test-object-1' -> pg 5.74dc35e2 (5.2) -> up ([1,0,2], p1) acting ([1,0,2], p1)
#删除Mytest pool，需要删除两次，让你确认。
[root@node2 ~]# ceph osd pool rm mytest
Error EPERM: WARNING: this will *PERMANENTLY DESTROY* all data stored in pool mytest.  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, followed by --yes-i-really-really-mean-it.
[root@node2 ~]# ceph osd map mytest test-object-1
osdmap e31 pool 'mytest' (5) object 'test-object-1' -> pg 5.74dc35e2 (5.2) -> up ([1,0,2], p1) acting ([1,0,2], p1)
```  

```
#将node1作为测试的机器，在上面先装上ceph
ceph-deploy install node1
ceph-deploy admin node1
```
这样，node1上就可以对集群执行操作，而不用每次登陆至集群，如
```
ssh node2 sudo ceph -s
```

[掘金上的这篇文章写得也比较好，可以参考](https://juejin.im/post/5b766acce51d4566877c1909)
至此，环境初步搭建完毕。


### 集群操作
通过systemd来管理
```
systemctl start ceph.target #start all daemons
systemctl stop ceph*.service ceph*.target
systemctl status ceph-osd@12 #检查osd.12的状态
systemctl status ceph*.service ceph*.target
```
按种类停止/启动一类daemon
```
systemctl stop ceph-mon\*.service ceph-mon.target
systemctl stop ceph-osd\*.service ceph-osd.target
systemctl stop ceph-mds\*.service ceph-mds.target

systemctl start/stop ceph-osd@{id}
systemctl start/stop ceph-mon@{hostname}
systemctl start/stop ceph-mds@{hostname}
```

### 健康检查

### 监控集群
```
ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status

ceph -s
```
除了每个守护进程有自己的日志，mointor上面的<font color=pink>/var/log/ceph/ceph.log</font>记录了整个系统高级别事件，同时也可以通过 <font color=pink>ceph -w </font>来监控。同时也可以用<font color=pink>ceph log last [n]</font>来查看最近n行的日志。

查看集群数据使用量以及在pool间的分布情况，用df命令，有点类似Linux的df <font color=pink>ceph df</font>

查看osd的状态  
osd的状态要么是在集群中（in the cluster）或是不在集群中（out）；并且，要么是正在运行的（up）或是停止运行的（down），如果在cluster中，就可读写；如果在它在集群中并且即将移除集群，ceph就会迁移pg到其他的osd；如果osd不在集群中(out)，crush将不会将pg绑到该osd上；如果osd是“out”的，那么它的状态也应该是out的；总而言之，它的状态有两个集合(in|out)和（up|down）

```
ceph osd stat
ceph osd dump
ceph osd tree
```
查看monitor集quorum状态
```
ceph quorum_status|python -m json.tool
```
查看Md5状态  
mds提供文件存储
```
ceph mds stat
#for i in node{2..4};do ssh $i sudo ceph mds stat;done
```
查看pg(placement group)状态  
pg映射对象到osd中

通过admin socket来查询  
路径为/var/run/ceph,ceph daemon {daemon-name} help


### 用户管理
ceph auth ls
```
[ceph@node1 my-cluster]$ ssh node2 sudo ceph auth ls
mds.node2
	key: AQBLa4Bc+IOQEBAAWniShT3OmiN/m7J6FDTKog==
	caps: [mds] allow
	caps: [mon] allow profile mds
	caps: [osd] allow rwx
osd.0
	key: AQCci39cdpzYERAA65jsvRJDbW1VIaTN/Tjxvw==
	caps: [mgr] allow profile osd
	caps: [mon] allow profile osd
	caps: [osd] allow *
  ...
```
输出格式：type.id，以上mds是类型，node2是id;osd是类型，0是id；接着是key,value认证，caps定义了权限范围

查单个用户

ceph auth get {type.id}

加一个用户

```
ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring
ceph auth get-or-create-key client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key
```
如果只提供了用户的osd权限，而没有限制进入特定pool，用户就有进入所有集群的Pools的权限。

删一个用户
...

keyring管理   
默认位置：/etc/ceph/$cluster.$name.keyring /etc/ceph/$cluster.keyring /etc/ceph/keyring /etc/ceph/keyring.bin



### Pools


首先什么是POOLS?

当你部署了一个集群，却并没有创建一个pool时，ceph会使用默认的pool来存储数据。一个pool能给你提供：
1. Resilience: 还原能力，你可以设置允许多少OSD来避免丢失数据，对于replicated pools，一个对象的副本是固定数量。典型的配置是size=2,只有一个副本。

2. Placement Groups：你可以为这个Pool设置placement groups的数量。一个典型的配置是一个osd，100个placement，来提供最佳的平衡避免使用太多的计算资源。当有多个pool时，要保证给pool整体上有一个合理数量的 placement groups

3. CRUSH rules
4. snapshots: 快照：ceph osd pool mksnap，其实是对部分pool做了快照

> 从Luminous开始，所有的pools需要绑定到应用

list
```
ceph osd lspools
```
创建
```
ceps osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] \
     [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} {pg-num}  {pgp-num}   erasure \
          [erasure-code-profile] [crush-rule-name] [expected_num_objects
```
<font color=green>绑定pool到应用</font>    
pool在使用之前需要与应用绑定。文件系统的pools和通过RGW对象存储自动创建的pools都会自动绑定；块存储（RBD）的pools，需要通过rbd工具对它初始化。  
其他场景，你可以手动绑定一个自由格式应用名到一个Pool

```
ceph osd pool application enable {pool-name} {application-name}
```
>注意：<font color=red>CephFs的应用名application name是cephfs,RBD的应用名是rbd，RGW的应用名是rgw</font>

设置pool quota

删除pool

重命名pool

查看pool的统计
```
rados df
```
快照

移除快照

给Pool设置一个Key/value

获取pool的value



### 文件存储

![cephFS](/styles/images/cephFS.png)







```
#在admin节点上，用ceph工具创建一个pool，建议用rbd
#在admin节点，用rbd工具对pool作初始化
rbd pool init <pool-name>

# 创建block device image
#rbd create foo --size 4096 --image-feature layering [-m {mon-IP}] [-k /path/to/ceph.client.admin.keyring] [-p {pool-name}]

#映射image到block device
sudo rbd map foo --name client.admin [-m {mon-IP}] [-k /path/to/ceph.client.admin.keyring] [-p {pool-name}]
#用块设备创建文件系统
sudo mkfs.ext4 -m0 /dev/rbd/{pool-name}/foo
#挂载
sudo mkdir /mnt/ceph-block-device
sudo mount /dev/rbd/{pool-name}/foo /mnt/ceph-block-device
cd /mnt/ceph-block-device
#最后，实现开机自动挂载
```
