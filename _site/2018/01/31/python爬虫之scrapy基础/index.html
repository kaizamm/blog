<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>python爬虫之scrapy基础 - Think Deep,Work Lean</title>

	<link rel="shortcut icon" href="/styles/images/favicon.jpg">
	<link rel="icon" href="/styles/images/favicon.jpg">

	<link rel="stylesheet" href="/styles/css/index.css">
	<link rel="stylesheet" href="/styles/css/fontawesome/css/font-awesome.min.css">
	<link rel="stylesheet" href="/styles/css/syntax.css">
	<link rel="canonical" href="/2018/01/31/python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E5%9F%BA%E7%A1%80/">
	<link rel="alternate" type="application/rss+xml" title="Think Deep,Work Lean" href="/feed.xml">
	
	<meta name="keywords" content="python爬虫之scrapy基础, Think Deep,Work Lean, 张凯:逆水行舟,不进则退;取法乎上，仅得其中；取法乎中，仅得其下;究天人之际，通古今之变，成一家之言">
	<meta name="description" content="张凯:逆水行舟,不进则退;取法乎上，仅得其中；取法乎中，仅得其下;究天人之际，通古今之变，成一家之言">

	<script src="/styles/js/jquery.min.js"></script>
	<!--[if lt IE 9]>
    	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  	<![endif]-->
  	<script>
		var _hmt = _hmt || [];
		(function() {
		  var hm = document.createElement("script");
		  hm.src = "//hm.baidu.com/hm.js?a81273dded286ab83c533a4184e6ae8c";
		  var s = document.getElementsByTagName("script")[0]; 
		  s.parentNode.insertBefore(hm, s);
		})();
	</script>
  	<style type="text/css">
	  	.docs-content{
	  		margin-bottom: 10px;
	  	}
  	</style>
</head>

  <body class="index">

    <header class="navbar navbar-inverse navbar-fixed-top docs-nav" role="banner">
  <div class="container">
    <div class="navbar-header">
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a href="/" class="navbar-brand">
        <img src="/styles/images/logo.jpg">
      </a>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
      <ul class="nav navbar-nav">    
        <li>
          <a href="/">Home</a>
        </li>
        <li>
          <a href="/categories/">大类分解</a>
        </li>
        <li>
          <a href="/tag">小类内聚</a>
        </li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
            <a><span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span></a>
        </li>
        <li>
          <a href="/donate/"><strong>打赏</strong></a>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown">关于<b class="caret"></b></a>
          <ul class="dropdown-menu">
            <li><a rel="nofollow" target="_blank" href="https://github.com/kaizamm">Github</a></li>
            <li><a rel="nofollow" target="_blank" href="">关于作者</a></li>
            <li><a rel="nofollow" href="/books">我的书单</a></li>
            <li><a rel="nofollow" href="http://www.hifreud.com/domains/">域名管理</a></li>
            <li><a rel="nofollow" href="/reference">推荐博客</a></li>
            <li><a href="/feed.xml">RSS订阅</a></li>
            <li class="divider"></li>
            <li><a rel="nofollow" target="_blank" href="https://github.com/luoyan35714/LessOrMore.git">本项目</a></li>
          </ul>
        </li>
      </ul>
    </nav>
  </div>
</header>
    <div class="docs-header" id="content">
  <div class="container">
  	
  		<!--
		    <h1>python爬虫之scrapy基础</h1>
		    <p>Post on Jan 31, 2018 by <a href="/about">Kaiz</a></p>
		-->
		    <h1>Think Deep,Work Lean</h1>
    
  </div>
</div>
    
      
<div class="banner">
  <div class="container">
  	
    	<a href="/categories/#document-ref">document</a>	/
    	<a href="/tag/#python-ref">python</a>
    
  </div>
</div>

    

    <div class="container docs-container">
  <div class="row">
    <div class="col-md-3">
      <div class="sidebar hidden-print" role="complementary">
        <div id="navigation">
  <h1>目录</h1>
  <ul class="nav sidenav">
  </ul>
  <div style="height: 200px;width: 200px;">
    <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5ytn1ssq6za&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"> 
    </script>
  </div>
</div>

 
      </div>
    </div>
    <div class="col-md-9" role="main">
      <div class="panel docs-content">
        <div class="wrapper">
            <header class="post-header">
              <h1 class="post-title">python爬虫之scrapy基础</h1>
              <!--
                <p class="post-meta">Jan 31, 2018</p>
              -->
              <div class="meta">Posted on <span class="postdate">Jan 31, 2018</span> By <a target="_blank" href="https://kaizamm.github.com">Kaiz</a></div>
              <br />
            </header>
            <article class="post-content">
              <h3 id="前言">前言</h3>
<p>爬虫实现方式有很多，可选择的语言也很多，如node.js、java、python；对于python，实现的方式也有很多；可以直接用requests模块请求到网页数据后，用正则/xpath/css去匹配，涉及到的模块有bs4/urllib/urlencode/etree等；现在用scrapy模块，帮我们省去了数据获取的工作，我们只需要对数据进行清洗过滤即可，“伟人是站在巨人肩头的侏儒”，正所谓“工欲善其事，必先得其器”。Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<h3 id="工程搭建">工程搭建</h3>
<p>直接上官网 https://scrapy.org/ 和
http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html</p>
<h4 id="环境">环境</h4>
<ul>
  <li>Python 2.7 or Python 3.4+</li>
  <li>install
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>scrapy
</code></pre></div>    </div>
    <blockquote>
      <p>另外插一句，pip和conda的区别，自行百度吧; 关于scrapy有两种部署模式，部署到本地和部署到cloud；本次只记录本地</p>
    </blockquote>
  </li>
</ul>

<h4 id="编写spider">编写spider</h4>
<ul>
  <li>对于response数据过滤两种方式:xpath/css</li>
</ul>

<p>用css表达式:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># myspider.py</span>
<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">BlogSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'blogspider'</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'https://blog.scrapinghub.com'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">title</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'h2.entry-title'</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s">'title'</span><span class="p">:</span> <span class="n">title</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'a ::text'</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()}</span>

        <span class="k">for</span> <span class="n">next_page</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'div.prev-post &gt; a'</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</code></pre></div></div>
<p>运行</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@kaiz-virtual-machine:/opt# scrapy runspider myspider.py
2018-01-31 15:17:42 <span class="o">[</span>scrapy.utils.log] INFO: Scrapy 1.5.0 started <span class="o">(</span>bot: scrapybot<span class="o">)</span>
2018-01-31 15:17:42 <span class="o">[</span>scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 2.7.12 <span class="o">(</span>default, Nov 20 2017, 18:23:56<span class="o">)</span> - <span class="o">[</span>GCC 5.4.0 20160609],
...
2018-01-31 17:24:17 <span class="o">[</span>scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://blog.scrapinghub.com/page/11/&gt;
<span class="o">{</span><span class="s1">'title'</span>: u<span class="s1">'Scrapy 0.12 released'</span><span class="o">}</span>
2018-01-31 17:24:17 <span class="o">[</span>scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://blog.scrapinghub.com/page/11/&gt;
<span class="o">{</span><span class="s1">'title'</span>: u<span class="s1">'Spoofing your Scrapy bot IP using tsocks'</span><span class="o">}</span>
...
</code></pre></div></div>

<p>xpath语法详见http://www.runoob.com/xpath/xpath-syntax.html ;对于以上spider用xpath写为:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="k">class</span> <span class="nc">BlogSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'blogspider'</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'https://blog.scrapinghub.com'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">title</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">"//h2[@class='entry-title']/a/text()"</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s">'title'</span><span class="p">:</span> <span class="n">title</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">next_page</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'div.prev-post &gt; a'</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="编写第一个爬虫spider">编写第一个爬虫(Spider)</h3>
<p>现在开始用创建项目方式来爬虫，一般分为以下几个步骤：</p>
<ul>
  <li>创建一个scrapy项目</li>
  <li>定义提取的Item</li>
  <li>编写爬取网站的spider并提取Item</li>
  <li>编写Item Pipeline来存储提取到的Item(即数据)</li>
</ul>

<h4 id="创建项目">创建项目</h4>
<p>在开始爬取之前，需创建一个项目：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy startproject tutorial
</code></pre></div></div>
<ul>
  <li>scrapy.cfg: 项目的配置文件</li>
  <li>tutorial/: 该项目的python模块。之后您将在此加入代码。</li>
  <li>tutorial/items.py: 项目中的item文件.</li>
  <li>tutorial/pipelines.py: 项目中的pipelines文件.</li>
  <li>tutorial/settings.py: 项目的设置文件.</li>
  <li>tutorial/spiders/: 放置spider代码的目录.</li>
</ul>

<h4 id="item">Item</h4>
<p>爬取的主要目的就是从非结构性的数据源提取结构性数据；类似orm，或是java的实体类一样</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="k">class</span> <span class="nc">DmozItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="spider">spider</h4>
<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。是用户编写用于从一个单一的网站或是一类网站爬取到的数据的类，需继承scrapy.Spider类，有以下三个属性</p>
<ul>
  <li>name:用于区别Spider,惟一</li>
  <li>allowed_domains： 可选。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。</li>
  <li>start_urls： 目标url列表</li>
  <li>start_requests()： 该方法的默认实现是使用 start_urls 的url生成Request。当spider启动爬取并且未制定URL时，该方法被调用。 当指定了URL时，make_requests_from_url() 将被调用来创建Request对象。 该方法仅仅会被Scrapy调用一次，因此您可以将其实现为生成器。该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取的第一个Request。
如果您想要修改最初爬取某个网站的Request对象，您可以重写(override)该方法。 例如，如果您需要在启动时以POST登录某个网站，你可以这么写:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span><span class="s">"http://www.example.com/login"</span><span class="p">,</span>
                             <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">'user'</span><span class="p">:</span> <span class="s">'john'</span><span class="p">,</span> <span class="s">'pass'</span><span class="p">:</span> <span class="s">'secret'</span><span class="p">},</span>
                             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_in</span><span class="p">)]</span>
<span class="k">def</span> <span class="nf">logged_in</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
  <span class="c"># here you would extract links to follow and return Requests for</span>
  <span class="c"># each of them, with another callback</span>
  <span class="k">pass</span>
</code></pre></div>    </div>
  </li>
  <li>make_requests_from_url(url):该方法接受一个URL并返回用于爬取的 Request 对象。 该方法在初始化request时被 start_requests() 调用，也被用于转化url为request。默认未被复写(overridden)的情况下，该方法返回的Request对象中， parse() 作为回调函数，dont_filter参数也被设置为开启。 (详情参见 Request).</li>
  <li>parse()： 是Spider的一个方法。被调用时，每个初始URL完成下载后生成的response对象会作为唯一参数传递给该函数。该方法负责解析返回的数据(response data),提取数据生成item及生成需进一步处理的URL的requset对象。以下dmoz_spider.py:</li>
  <li>log(message[, level, component])
使用 scrapy.log.msg() 方法记录(log)message。 log中自动带上该spider的 name 属性。 更多数据请参见 Logging 。</li>
</ul>

<p>样例一：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"dmoz"</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">'dmoz.org'</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html"</span><span class="p">,</span>
         <span class="s">"http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/feed-exports.html"</span>
    <span class="p">]</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">response</span><span class="p">):</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</code></pre></div></div>

<p>样例二：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'example.com'</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">'example.com'</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">'http://www.example.com/1.html'</span><span class="p">,</span>
        <span class="s">'http://www.example.com/2.html'</span><span class="p">,</span>
        <span class="s">'http://www.example.com/3.html'</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">'A response from </span><span class="si">%</span><span class="s">s just arrived!'</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</code></pre></div></div>

<p>样例三：另一个在单个回调函数中返回多个Request以及Item的例子:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">MyItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'example.com'</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">'example.com'</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">'http://www.example.com/1.html'</span><span class="p">,</span>
        <span class="s">'http://www.example.com/2.html'</span><span class="p">,</span>
        <span class="s">'http://www.example.com/3.html'</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">sel</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//h3'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">h3</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//a/@href'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="爬取规则crawling-rules">爬取规则(Crawling rules)</h4>
<p>当编写爬虫规则时，请避免使用 parse 作为回调函数。 由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果 您覆盖了 parse 方法，crawl spider 将会运行失败。link_extractor 是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。callback 是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中每获取到链接时将会调用该函数。该回调函数接受一个response作为其第一个参数， 并返回一个包含 Item 以及(或) Request 对象(或者这两者的子类)的列表(list)。
http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy crawl dmoz
'''
2018-01-31 19:34:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-01-31 19:34:40 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://scrapy-chs.readthedocs.io/robots.txt&gt; (referer: None)
2018-01-31 19:34:40 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/feed-exports.html&gt; (referer: None)
2018-01-31 19:34:44 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html&gt; (referer: None)
'''
</code></pre></div></div>

<h3 id="selectors选择器">Selectors选择器</h3>
<p>当抓取网页时，你做的最常见的任务是从HTML源码中提取数据。现有的一些库可以达到这个目的：</p>

<p>这里给出XPath表达式的例子及对应的含义:</p>
<ul>
  <li>BeautifulSoup
是在程序员间非常流行的网页分析库，它基于HTML代码的结构来构造一个Python对象， 对不良标记的处理也非常合理，但它有一个缺点：慢</li>
  <li>lxml 是一个基于 ElementTree (不是Python标准库的一部分)的python化的XML解析库(也可以解析HTML)。</li>
</ul>

<p>Scrapy提取数据有自己的一套机制。它们被称作选择器(seletors)，因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML文件中的某个部分。</p>

<p>XPath 是一门用来在XML文件中选择节点的语言，也可以用在HTML上。 CSS 是一门将HTML文档样式化的语言。选择器由它定义，并与特定的HTML元素的样式相关连。</p>
<ul>
  <li>/html/head/title: 选择HTML文档中 &lt;head&gt; 标签内的 <title> 元素</title></li>
  <li>/html/head/title/text(): 选择上面提到的 <title> 元素的文字</title></li>
  <li>//td: 选择所有的 &lt;td&gt; 元素</li>
  <li>//div[@class=”mine”]: 选择所有具有 class=”mine” 属性的 div 元素</li>
</ul>

<p>Scrapy选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。</p>

<h5 id="构造选择器selectors">构造选择器(selectors)</h5>
<p>Scrapy selector是以 文字(text) 或 TextResponse 构造的 Selector 实例。 其根据输入的类型自动选择最优的分析方法(XML vs HTML):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
<span class="c">#以文字构造:</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">body</span> <span class="o">=</span> <span class="s">'&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">body</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//span/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'good'</span><span class="p">]</span>
<span class="c"># 以response构造:</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">'http://example.com'</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//span/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'good'</span><span class="p">]</span>
<span class="c">#为了方便起见，response对象以 .selector 属性提供了一个selector， 您可以随时使用该快捷方法:</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//span/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'good'</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="在shell中尝试selector选择器">在shell中尝试selector选择器</h4>
<p>HTML源码:</p>
<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;html&gt;</span>
 <span class="nt">&lt;head&gt;</span>
  <span class="nt">&lt;base</span> <span class="na">href=</span><span class="s">'http://example.com/'</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;title&gt;</span>Example website<span class="nt">&lt;/title&gt;</span>
 <span class="nt">&lt;/head&gt;</span>
 <span class="nt">&lt;body&gt;</span>
  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">'images'</span><span class="nt">&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">'image1.html'</span><span class="nt">&gt;</span>Name: My image 1 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">'image1_thumb.jpg'</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">'image2.html'</span><span class="nt">&gt;</span>Name: My image 2 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">'image2_thumb.jpg'</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">'image3.html'</span><span class="nt">&gt;</span>Name: My image 3 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">'image3_thumb.jpg'</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">'image4.html'</span><span class="nt">&gt;</span>Name: My image 4 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">'image4_thumb.jpg'</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">'image5.html'</span><span class="nt">&gt;</span>Name: My image 5 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">'image5_thumb.jpg'</span> <span class="nt">/&gt;&lt;/a&gt;</span>
  <span class="nt">&lt;/div&gt;</span>
 <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</code></pre></div></div>
<p>首先, 我们打开shell:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy shell <span class="s1">'http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html'</span>
</code></pre></div></div>
<p>接着，当shell载入后，您将获得名为 response 的shell变量，其为响应的response， 并且在其 response.selector 属性上绑定了一个selector。</p>

<ul>
  <li>response.body</li>
  <li>response.headers</li>
  <li>response.selector:当输入response.selector时，将获取到一个可以用于查询数据的selector选择器，以及映射到response.selector.xpath()、response.selector.css()的快捷方法shortcut:response.xpath()和response.css()</li>
  <li>response.url</li>
</ul>

<p>可用的Scrapy对象</p>

<p>Scrapy终端根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象及 Selector 对象(对HTML及XML内容)。</p>

<p>这些对象有:</p>
<ul>
  <li>crawler - 当前 Crawler 对象.</li>
  <li>spider - 处理URL的spider。 对当前URL没有处理的Spider时则为一个 + Spider 对象。</li>
  <li>request - 最近获取到的页面的 Request 对象。 您可以使用 replace() 修+ 改该request。或者 使用 fetch 快捷方式来获取新的request。</li>
  <li>response - 包含最近获取到的页面的 Response 对象。</li>
  <li>sel - 根据最近获取到的response构建的 Selector 对象。</li>
  <li>settings - 当前的 Scrapy settings</li>
</ul>

<p>我们构建一个XPath来选择title标签内的文字:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//title/text()'</span><span class="p">)</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">Selector</span> <span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="n">xpath</span><span class="o">=//</span><span class="n">title</span><span class="o">/</span><span class="n">text</span><span class="p">()</span><span class="o">&gt;</span><span class="p">]</span>
</code></pre></div></div>
<p>由于在response中使用XPath、CSS查询十分普遍，因此，Scrapy提供了两个实用的快捷方式: response.xpath() 及 response.css():</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//title/text()'</span><span class="p">)</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">Selector</span> <span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="n">xpath</span><span class="o">=//</span><span class="n">title</span><span class="o">/</span><span class="n">text</span><span class="p">()</span><span class="o">&gt;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'title::text'</span><span class="p">)</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">Selector</span> <span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="n">xpath</span><span class="o">=//</span><span class="n">title</span><span class="o">/</span><span class="n">text</span><span class="p">()</span><span class="o">&gt;</span><span class="p">]</span>
</code></pre></div></div>
<p>如你所见， .xpath() 及 .css() 方法返回一个类 SelectorList 的实例, 它是一个新选择器的列表。这个API可以用来快速的提取嵌套数据。</p>

<p>为了提取真实的原文数据，你需要调用 .extract() 方法如下</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//title/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'Example website'</span><span class="p">]</span>
</code></pre></div></div>

<p>现在我们将得到根URL(base URL)和一些图片链接:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//base/@href'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'http://example.com/'</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'base::attr(href)'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'http://example.com/'</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//a[contains(@href, "image")]/@href'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'image1.html'</span><span class="p">,</span>
 <span class="s">u'image2.html'</span><span class="p">,</span>
 <span class="s">u'image3.html'</span><span class="p">,</span>
 <span class="s">u'image4.html'</span><span class="p">,</span>
 <span class="s">u'image5.html'</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'a[href*=image]::attr(href)'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'image1.html'</span><span class="p">,</span>
 <span class="s">u'image2.html'</span><span class="p">,</span>
 <span class="s">u'image3.html'</span><span class="p">,</span>
 <span class="s">u'image4.html'</span><span class="p">,</span>
 <span class="s">u'image5.html'</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//a[contains(@href, "image")]/img/@src'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'image1_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image2_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image3_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image4_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image5_thumb.jpg'</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">'a[href*=image] img::attr(src)'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="p">[</span><span class="s">u'image1_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image2_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image3_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image4_thumb.jpg'</span><span class="p">,</span>
 <span class="s">u'image5_thumb.jpg'</span><span class="p">]</span>
</code></pre></div></div>
<h4 id="结合正则表达式使用选择器selectors">结合正则表达式使用选择器(selectors)</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//a[contains(@href, "image")]/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s">r'Name:\s*(.*)'</span><span class="p">)</span>
<span class="p">[</span><span class="s">u'My image 1'</span><span class="p">,</span>
 <span class="s">u'My image 2'</span><span class="p">,</span>
 <span class="s">u'My image 3'</span><span class="p">,</span>
 <span class="s">u'My image 4'</span><span class="p">,</span>
 <span class="s">u'My image 5'</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="使用相对xpaths">使用相对XPaths</h4>
<p>记住如果你使用嵌套的选择器，并使用起始为 / 的XPath，那么该XPath将对文档使用绝对路径，而且对于你调用的 Selector 不是相对路径。</p>

<p>比如，假设你想提取在 &lt;div&gt; 元素中的所有 &lt;p&gt; 元素。首先，你将先得到所有的 &lt;div&gt; 元素:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">divs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//div'</span><span class="p">)</span>
</code></pre></div></div>
<p>开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 &lt;div&gt; 元素内部提取所有的 &lt;p&gt; 元素:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//p'</span><span class="p">):</span>  <span class="c"># this is wrong - gets all &lt;p&gt; from the whole document</span>
<span class="o">...</span>     <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</code></pre></div></div>
<p>下面是比较合适的处理方法(注意 .//p XPath的点前缀):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'.//p'</span><span class="p">):</span>  <span class="c"># extracts all &lt;p&gt; inside</span>
<span class="o">...</span>     <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</code></pre></div></div>
<p>另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'p'</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</code></pre></div></div>
<h3 id="可用的工具命令">可用的工具命令</h3>
<p>scrapy提供了两种类型的命令。一种必须在scrapy项目中运行，别外一种是全局的</p>
<ul>
  <li>全局命令
    <ul>
      <li>startproject</li>
      <li>settings 例如获取项目名称：scrapy settings –get BOT_NAME</li>
      <li>runspider:这个命令和crawl命令的区别在于crawl命令后是spider的name，而runspider命令后加的是爬虫的文件名，在本文的项目中，使用crawl命令：scrapy crawl baidu; 使用runspider就是：scrapy runspider baidu.py</li>
      <li>shell</li>
      <li>fetch: –nolog/–headers/–no-redirect 分别是不输出日志信息，返回网页的请求头和禁止重定向。如果网页没有重定向的话返回的还是原网页。
使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。 scrapy fetch –nolog http://www.example.com/some/page.htm</li>
      <li>view: 这个命令比较有用，它的作用是请求网址，输出网址的源码，并将该网页保存成一个文件，使用浏览器打开。如果打开的网址和你正常加载的网页有所不同，一般情况下没显示的部分使用了异步加载。因此该命令可以用来检查 spider 所获取到的页面,并确认这是您所期望的。</li>
      <li>version: 这个命令可以查询当前scrapy的版本，和一些依赖库版本信息。</li>
    </ul>
  </li>
  <li>项目命令
    <ul>
      <li>crawl: 使用spider进行爬取 scrapy crawl myspider</li>
      <li>check: 运行contract检查 scrapy check [-l] <spider></spider></li>
      <li>list: 列出当前项目中所有可用的spider。每行输出一个spider。scrapy list</li>
      <li>edit: 使用 EDITOR 中设定的编辑器编辑给定的spider语法: scrapy edit <spider>; 如果你不使用vim作为编辑器的话，这个命令不常用，因为这个命令会调用vim来编辑文件。</spider></li>
      <li>parse: 获取给定的 URL 并使用相应的 spider 分析处理。如果您提供 –callback 选项,则使用 spider 的该方法处理,否则使用 parse</li>
      <li>genspider:在spiders目录下创建最基本的模版，scrapy genspider baidu www.baidu.com
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy genspider <span class="nt">-l</span>
<span class="s1">'''
Available templates:
basic
crawl
csvfeed
xmlfeed
'''</span>
 scrapy genspider <span class="nt">-d</span> basic
 scrapy genspider <span class="nt">-t</span> basic example example.com
</code></pre></div>        </div>
      </li>
      <li>deploy</li>
      <li>bench</li>
    </ul>
  </li>
</ul>

<h3 id="还有什么">还有什么?</h3>
<p>到此你已经从网页上抓到了你想要的一些数据,但这仅仅冰山一角.scrapy还提供了更为强大的特性使得爬虫更为简单,如:</p>
<ul>
  <li>html,xml源数据选择及提取内置支持</li>
  <li>提供了一系列在spider之间共享的可复用的过滤器,即Item Loaders. 对智能处理爬取数据提供了内置支持</li>
  <li>通过feed导出提供了多种格式JSON,CSV,XML. 多存储后端FTP,S3,本地文件系统的支持</li>
  <li>提供了media pipeline,可以自动爬取到的数据中的图片或其他资源</li>
  <li>高可扩展性,你可以通过使用signals,设计好的API(中间件,extensions,pipelines)来定制你的功能.</li>
  <li>内置中间件及扩展为下列功能提供了支持
    <ul>
      <li>cookies and session处理</li>
      <li>HTTP压缩</li>
      <li>HTTP认证</li>
      <li>user-agent模拟</li>
      <li>robots.txt</li>
      <li>爬取深度限制</li>
    </ul>
  </li>
  <li>针对非英语系中不标准或者错误的编码声明，提供了自动检测以健壮的编码支持</li>
  <li>支持根据模板生成爬虫。在加速爬虫创建同时，保持在大型项目中的代码更为一致。详细内容见genspider命令 http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html#std:command-genspider</li>
  <li>针对多爬虫下性能评估、失败检测，提供了可扩展的状态收集工具</li>
  <li>提供交互式shell终端，为您测试xpath表达式、编写和调试爬虫提供了极大的方便</li>
  <li>提供system service，简化在生产环境的部署及运行</li>
  <li>内置web service使您可以监视及控制您的机器</li>
  <li>内置telnet终端，通过在scrapy进程中钩入python终端，例您可以查看并且调试爬虫</li>
  <li>Logging为您在爬取过程中捕捉错误提供了方便</li>
  <li>支持sitemaps爬取</li>
  <li>具有缓存的DNS解析器</li>
</ul>

            </article>
        </div>
      </div>
    </div>
  </div>
</div>

    
    <footer class="footer" role="contentinfo">
	<div class="container">
		<p class="copyright">Copyright &copy; 2014-2018 <a href=""><code>Kaiz</code></a>.</p>
		<p>Powered by <a href="http://jekyllrb.com">Jekyll</a>, themed from <a href="http://lesscss.cn/">Less</a>, refactored by <a href="http://www.hifreud.com/">Freud Kang</a></p>
	</div>
</footer>

<script src="/styles/js/jquery.min.js"></script>
<script src="/styles/js/bootstrap.min.js"></script>
<script src="/styles/js/holder.min.js"></script>
<script src="/styles/js/lessismore.js"></script>
<script src="/styles/js/application.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  </body>
</html>
